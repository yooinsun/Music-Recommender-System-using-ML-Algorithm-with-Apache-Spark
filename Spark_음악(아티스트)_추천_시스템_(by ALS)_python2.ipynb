{
 "metadata": {
  "name": "Spark_음악(아티스트)_추천_시스템_(by ALS)_python",
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Import \n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F #from pyspark.sql.functions import round,udf\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, CrossValidatorModel\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from cassandra.concurrent import execute_concurrent_with_args\n",
    "import pyspark.ml.recommendation\n",
    "from cassandra.cluster import Cluster\n",
    "from pyspark.sql.types import IntegerType"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T08:01:05.525826Z",
     "start_time": "2024-03-18T08:01:05.122037Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Spark session 생성\n",
    "conf = SparkConf()\n",
    "conf.set('spark.app.name', 'recomm_music') #어플리케이션 이름, UI에 표시됨\n",
    "conf.set(\"spark.jars.packages\", 'com.datastax.spark:spark-cassandra-connector_2.12:3.2.0')\n",
    "conf.set(\"spark.jars.packages\", 'com.redislabs:spark-redis_2.12:3.1.0')\n",
    "conf.set(\"spark.redis.connection.host\", \"127.0.0.1\")\n",
    "conf.set(\"spark.redis.connection.port\", \"6379\")\n",
    "conf.set(\"spark.cassandra.connection.host\", \"127.0.0.1\")\n",
    "conf.set(\"spark.cassandra.connection.port\", \"9042\")\n",
    "# conf.set(\"spark.sql.extensions\", \"com.datastax.spark.connector.CassandraSparkExtensions\")\n",
    "# conf.set(\"spark.sql.catalog.mycatalog\" , \"com.datastax.spark.connector.datasource.CassandraCatalog\")\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "#spark = SparkSession.builder.appName('recomm_music').getOrCreate()\n",
    "\n",
    "#%load_ext sparksql_magic"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T08:02:00.323236Z",
     "start_time": "2024-03-18T08:02:00.276105Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Spark session 생성\n",
    "# conf = SparkConf()\n",
    "# conf.set('spark.app.name', 'recomm_music') #어플리케이션 이름, UI에 표시됨\n",
    "# conf.set(\"spark.jars.packages\", 'com.datastax.spark:spark-cassandra-connector_2.12:3.2.0')\n",
    "# conf.set(\"spark.jars.packages\", 'com.redislabs:spark-redis_2.12:3.1.0')\n",
    "# spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "spark = SparkSession.builder.appName('recomm_music').getOrCreate()\n",
    "\n",
    "%load_ext sparksql_magic\n",
    "# %config SparkSql.limit= 1000"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2.4\n",
      "local[*]\n",
      "yooinsun\n",
      "hadoop version = 3.3.1\n"
     ]
    }
   ],
   "source": [
    "# Spark 버전 및 정보 확인\n",
    "print(spark.version)\n",
    "print(spark.sparkContext.master)\n",
    "print(spark.sparkContext.sparkUser())\n",
    "print(f'hadoop version = {spark._jvm.org.apache.hadoop.util.VersionInfo.getVersion()}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T17:42:18.884932Z",
     "start_time": "2024-03-17T17:42:18.816961Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---------+--------+\n",
      "| userid|artistid|playcount|playyear|\n",
      "+-------+--------+---------+--------+\n",
      "|2064012|    4468|   439771|   3.347|\n",
      "|1059637| 1002095|   433060|   3.296|\n",
      "|1059637| 1026440|   155895|   1.186|\n",
      "|2069889| 1002095|   101076|   0.769|\n",
      "|2020513| 1007801|    88908|   0.677|\n",
      "|1073421| 1004440|    67548|   0.514|\n",
      "+-------+--------+---------+--------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 17:02:44,947 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "2024-03-18 17:02:44,956 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "2024-03-18 17:02:46,081 WARN netlib.InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "/Users/yooinsun/project_Recom_Music_Spark/venv/lib/python3.9/site-packages/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "userArtistDS = spark.read.text(\"hdfs://localhost:9000/data/audio/profiledata_06-May-2005/user_artist_data.txt\")\n",
    "\n",
    "userArtistCSVDF = spark.read.option(\"sep\", \" \").csv(\"hdfs://localhost:9000/data/audio/profiledata_06-May-2005/user_artist_data.txt\").toDF(\"userid\", \"artistid\", \"playcount\")\n",
    "\n",
    "userArtistCSVDF2 = userArtistCSVDF.selectExpr(\"cast(userid as int)\", \"cast(artistid as int)\", \"cast(playcount as int)\")\n",
    "\n",
    "\n",
    "userArtistCSVDF2.where(userArtistCSVDF2.playcount > 64800).withColumn('playyear', F.round(*[userArtistCSVDF2.playcount * 4 / 60/ 24/365] , 3)).orderBy('playcount', ascending=False).show()\n",
    "\n",
    "userArtistCSVDF3 = userArtistCSVDF2.filter(userArtistCSVDF2.playcount <= 64800)\n",
    "\n",
    "artistDS = spark.read.text(\"hdfs://localhost:9000/data/audio/profiledata_06-May-2005/artist_data.txt\")\n",
    "\n",
    "artistDF = spark.read.option(\"sep\", \"\\t\").option(\"inferSchema\", True).csv(\"hdfs://localhost:9000/data/audio/profiledata_06-May-2005/artist_data.txt\").toDF(\"artistid\", \"artistname\")\n",
    "\n",
    "@F.udf\n",
    "def str_to_int(s):\n",
    "    try:\n",
    "        return int(s)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "artistFinal = artistDF.withColumn(\"artistid\",str_to_int(artistDF.artistid)).where(\"artistid is not null\").where('artistname is not null').selectExpr(\"cast(artistid as int) artistid\", 'artistname')\n",
    "\n",
    "artistAliasDS = spark.read.text(\"hdfs://localhost:9000/data/audio/profiledata_06-May-2005/artist_alias.txt\")\n",
    "\n",
    "artistAliasDF = spark.read.option(\"sep\", \"\\t\").option(\"inferSchema\", True).csv(\"hdfs://localhost:9000/data/audio/profiledata_06-May-2005/artist_alias.txt\").toDF(\"badid\", \"goodid\") \n",
    "\n",
    "artistAliasFinal = artistAliasDF.filter(\"badid is not null\").filter(\"goodid is not null\")\n",
    "\n",
    "userArtistCSVDF3.createOrReplaceTempView(\"userArtistCSVDF3\")\n",
    "\n",
    "userArtistFinal = userArtistCSVDF3.join(artistAliasFinal, userArtistCSVDF3.artistid== artistAliasFinal.badid, \"left_outer\").withColumn(\"artistid2\", F.expr(\"case when badid is not null then goodid else artistid end\")).withColumn(\"artistid2\", F.when(F.col(\"badid\").isNotNull(), F.col(\"goodid\")).otherwise(F.col(\"artistid\"))).select('userid', F.col('artistid2').alias(\"artistid\"), 'playcount')\n",
    "\n",
    "userArtistFinal.createOrReplaceTempView(\"userArtistFinal\")\n",
    "\n",
    "userArtistFinal2 = userArtistFinal.groupBy(\"userid\", \"artistid\").agg(F.sum('playcount').alias('playcount')).filter('playcount <= 64800')\n",
    "\n",
    "try:\n",
    "    cluster = Cluster(['127.0.0.1'], port=9042)\n",
    "    session = cluster.connect(keyspace='mykeyspace')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "userArtistFinal3 = spark.createDataFrame(data=session.execute(\"SELECT * FROM user_artist_data\").all(), schema = ['userid', 'artistid', 'playcount'])\n",
    "\n",
    "\n",
    "\n",
    "als = ALS().setParams(rank=10,coldStartStrategy='drop',seed=11, alpha=40, regParam=0.1, maxIter=5, implicitPrefs=True, ratingCol='playcount', itemCol='artistid', userCol='userid') \n",
    "\n",
    "alsModel = als.fit(userArtistFinal3)\n",
    "\n",
    "userDS =spark.createDataFrame([1001440, 2010008, 987654321], IntegerType()).toDF('userID')\n",
    "# 특정 사용자를 위한 추천 5개....\n",
    "recommendedForSomeUsersDF = alsModel.recommendForUserSubset(userDS, 5)\n",
    "# explode....\n",
    "recommendedForSomeUsersDF2 = recommendedForSomeUsersDF.withColumn(\"recommend\", F.explode(\"recommendations\")).withColumn(\"artistid\", F.col('recommend').artistid).withColumn(\"rating\",  F.col(\"recommend\").rating)\n",
    "# 불필요한 colum drop하기\n",
    "recommendedForSomeUsersDF3 = recommendedForSomeUsersDF2.drop(\"recommendations\", \"recommend\")\n",
    "recommendedForSomeUsersDF4 = recommendedForSomeUsersDF3.join(artistFinal, 'artistid').orderBy(F.col(\"userid\").asc(), F.col(\"rating\").desc())\n",
    "\n",
    "predictionsDF = alsModel.transform(userArtistFinal3)\n",
    "\n",
    "regEval = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"playcount\", metricName=\"rmse\" )\n",
    "\n",
    "rmse = regEval.evaluate(predictionsDF)\n",
    "\n",
    "pipeline = Pipeline(stages=[als])#Pipeline.setStages(als)\n",
    "\n",
    "paramMaps = ParamGridBuilder().addGrid(als.alpha, (40.0, 10.0, 5.0, 1.0)).addGrid(als.rank, (2,3,10)).addGrid(als.regParam, (1.0, 0.01)).build()\n",
    "\n",
    "cv = CrossValidator(estimator=pipeline,estimatorParamMaps=paramMaps,evaluator=regEval,numFolds=2 )  \n",
    "\n",
    "trainDS, testDS = userArtistFinal3.randomSplit([0.7, 0.3], 11)\n",
    "trainDS.persist(StorageLevel.MEMORY_ONLY)\n",
    "testDS.persist(StorageLevel.MEMORY_ONLY)\n",
    "\n",
    "cvModel = cv.fit(trainDS)\n",
    "\n",
    "predictionsDF2 = cvModel.transform(testDS)\n",
    "\n",
    "rmse2 = regEval.evaluate(predictionsDF2)\n",
    "\n",
    "loadedBestALSModel = alsModel.load(\"hdfs://localhost:9000/model/audio/profiledata_06-May-2005/als\")\n",
    "recommendedForAllUsersDF = loadedBestALSModel.recommendForAllUsers(5)\n",
    "\n",
    "recommendedForAllUsersDF2 = recommendedForAllUsersDF.withColumn(\"recommendation\", F.explode(\"recommendations\")).withColumn(\"artistid\", F.col(\"recommendation.artistid\"))\n",
    "\n",
    "recommendedForAllUsersDF3 = recommendedForAllUsersDF2.groupBy(\"userid\").agg(F.collect_list(\"artistid\").alias(\"artistid_list\")).withColumn(\"recommended_artistids\", F.array_join(F.col(\"artistid_list\"), \" \"))\n",
    "\n",
    "recommendedForAllUsersDF4 = recommendedForAllUsersDF3.select( 'userid', 'recommended_artistids' )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T08:06:52.498283Z",
     "start_time": "2024-03-18T08:02:11.188177Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. 데이터 수집\n",
    "- 다운로드 및 HDFS 업로드\n",
    "- ```https://storage.googleapis.com/aas-data-sets/profiledata_06-May-2005.tar.gz```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!mkdir -p \"/Users/yooinsun/data/audio\"\n",
    "!wget -O /Users/yooinsun/data/audio/profiledata_06-May-2005.tar.gz https://storage.googleapis.com/aas-data-sets/profiledata_06-May-2005.tar.gz"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!tar xvfz ../data/audio/profiledata_06-May-2005.tar.gz -C ../data/audio\n",
    "!ls -alh ../data/audio/profiledata_06-May-2005"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#export HADOOP_USER_NAME=spark\n",
    "!../hadoop-3.3.6/bin/hdfs dfs -mkdir -p /data/audio\n",
    "!../hadoop-3.3.6/bin/hdfs dfs -put -f /data/audio/profiledata_06-May-2005 /data/audio\n",
    "\n",
    "!../hadoop-3.3.6/bin/hdfs dfs -ls -h /data/audio\n",
    "!../hadoop-3.3.6/bin/hdfs dfs -ls -h /data/audio/profiledata_06-May-2005"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!../hadoop-3.3.6/bin/hdfs dfs -cat /data/audio/profiledata_06-May-2005/README.txt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!../hadoop-3.3.6/bin/hdfs dfs -cat /data/audio/profiledata_06-May-2005/user_artist_data.txt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!../hadoop-3.3.6/bin/hdfs dfs -cat /data/audio/profiledata_06-May-2005/artist_data.txt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!../hadoop-3.3.6/bin/hdfs dfs -cat /data/audio/profiledata_06-May-2005/artist_alias.txt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. 데이터 탐색, 데이터 정렬\n",
    "\n",
    "\n",
    "```\n",
    "\"Music Listening Dataset\" (Audioscrobbler.com => https://last.fm/) \n",
    "-> 6 May 2005, for around 150,000 real people\n",
    "-> Audioscrobbler is receiving around 2 million song submissions per day\n",
    "(https://ko.wikipedia.org/wiki/%EB%9D%BC%EC%8A%A4%ED%8A%B8_FM, https://namu.wiki/w/Last.fm)\n",
    "''''\n",
    "scrobble\t미국·영국 [|skrɒbəl]  [VERB] (of an online music service) to record a listener's musical preferences and recommend similar music that he or she might enjoy\n",
    "```\n",
    "\n",
    "- user_artist_data.txt\n",
    "    - 3 columns: userid artistid playcount => 공백(' ')으로 구분됨\n",
    "    \n",
    "    - 형식 : ```유저(공백)아티스트(공백)플레이카운트```\n",
    "        - ex) 1052430 2032445 12 =>\t2032445\t신화 (Shinhwa)\n",
    "     \n",
    "- artist_data.txt\n",
    "    - 2 columns: artistid artist_name\t=> 탭('\\t')으로 구분됨\n",
    "    \n",
    "    - 형식 :  ```아티스트(탭)이름```\n",
    "        - ex) 한글 아티스트 이름 (다나, 클래지콰이, 윤건, 엠씨더맥스, 음악가 없음, 유미, 김성필, 엠 투 엠, 신부수업 OST, 데이슬리퍼, 신화 (Shinhwa), 베이비 복스, 윤도현)\n",
    "        - ex) 2032445\t신화 (Shinhwa)\n",
    "    \n",
    "- artist_alias.txt\n",
    "    - 2 columns: badid goodid\t=> 탭('\\t')으로 구분됨\n",
    "    - known incorrectly spelt artists and the correct artist id. you can correct errors in user_artist_data as you read it in using this file\n",
    "\n",
    "    - 형식 : ```배드아이디(탭)굿아이디```    \n",
    "        - ex) 2032445\t6834637\n",
    "        - ex) 2032445\t신화 (Shinhwa) : 6834637\t신화\n",
    "        - ex) 10103564\t베이비 복스 : 1101679\tBaby V.O.X\n",
    "        - ex) 2101369\tBaby Vox 3 : 1101679\tBaby V.O.X\n",
    "        - ex) 1028894\tBaby Vox : 1101679\tBaby V.O.X"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "user_artist_data.txt 정보 확인\n",
    "- text method 로 txt 읽기\n",
    "- csv method 로 txt 읽기\n",
    "- describe(), summary() 를 통해, 데이터 살펴보기"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# python\n",
    "userArtistDS = spark.read.text(\"hdfs://localhost:9000/data/audio/profiledata_06-May-2005/user_artist_data.txt\")\n",
    "\n",
    "print(userArtistDS.count())\n",
    "userArtistDS.printSchema()\n",
    "userArtistDS.show()\n",
    "\n",
    "# #scala\n",
    "# val userArtistDS = spark\n",
    "#     .read\n",
    "#     .textFile(\"hdfs://spark-master-01:9000/data/audio/profiledata_06-May-2005/user_artist_data.txt\")  //--spark.read.textFile(path)\n",
    "# \n",
    "# println(userArtistDS.count())\n",
    "# userArtistDS.printSchema()\n",
    "# userArtistDS.show()\n",
    "# z.show(userArtistDS.limit(20))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# python\n",
    "userArtistCSVDF = spark.read.option(\"sep\", \" \").csv(\"hdfs://localhost:9000/data/audio/profiledata_06-May-2005/user_artist_data.txt\").toDF(\"userid\", \"artistid\", \"playcount\")\n",
    "\n",
    "print(userArtistCSVDF.count())\n",
    "print(userArtistDS.count() - userArtistCSVDF.count())\n",
    "userArtistCSVDF.printSchema()\n",
    "userArtistCSVDF.show()\n",
    "\n",
    "# # scalar\n",
    "# val userArtistCSVDF = spark\n",
    "#     .read\n",
    "#     .option(\"sep\", \" \")\n",
    "#     .csv(\"hdfs://localhost:9000/data/audio/profiledata_06-May-2005/user_artist_data.txt\")  //--spark.read.csv(path)....\n",
    "#     .toDF(\"userid\", \"artistid\", \"playcount\")\n",
    "# \n",
    "# println(userArtistCSVDF.count())\n",
    "# println(userArtistDS.count() - userArtistCSVDF.count())\n",
    "# userArtistCSVDF.printSchema()\n",
    "# userArtistCSVDF.show()\n",
    "# z.show(userArtistCSVDF.limit(20))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "userArtistCSVDF.describe().show()  #--DataFrame.describe()...."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "userArtistCSVDF.summary().show()  #-DataFrame.summary()...."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "통계정보 확인 했을떄, 이상데이터 발견\n",
    "- 확인 방법: describe() 와 summary() 를 활용하여 데이터 통계 확인\n",
    "- 이상 데이터: min, max, 25%, 50%, 75% 수치가 비정상적임을 확인함\n",
    "- 조치 방안: 각 column type 을 string -> int 로 변경"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# python\n",
    "userArtistCSVDF2 = userArtistCSVDF.selectExpr(\"cast(userid as int)\", \"cast(artistid as int)\", \"cast(playcount as int)\")\n",
    "userArtistCSVDF2.printSchema()\n",
    "\n",
    "# scala\n",
    "# val userArtistCSVDF2 = userArtistCSVDF\n",
    "#     .selectExpr(\"cast(userid as int)\", \"cast(artistid as int)\", \"cast(playcount as int)\")  //--cast(col as type)....\n",
    "# \n",
    "# userArtistCSVDF2.printSchema"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# type(str->int) 변경 후, 통계정보 재확인\n",
    "userArtistCSVDF2.summary().show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "userArtistCSVDF2.where(\"playcount = 439771\").show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "[ playcount = 439771의 의미? ]\n",
    "\n",
    "=> 음악 한곡당 4분이라 가정할 경우, 쉬지않고 연속으로 약 3.34년 동안 음악을 들은 것\n",
    "'''\n",
    "\n",
    "print((4 * 439771) /60/ 24/ 365)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "[ 해당 프로젝트에서 max palycount 값 정하는 기준 ]\n",
    "\n",
    "- 기준은 6개월이 최대치라고 설정함 + 음악 한곡당 4분이라 가정함\n",
    "- max plyacount 값은 64800 으로 설정함\n",
    "'''\n",
    "\n",
    "print((60 * 24 * 30 * 6) / 4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "새로운 Column 추가하기"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# python\n",
    "userArtistCSVDF2.where(userArtistCSVDF2.playcount > 64800).withColumn('playyear', F.round(*[userArtistCSVDF2.playcount * 4 / 60/ 24/365] , 3)).orderBy('playcount', ascending=False).show()\n",
    "\n",
    "# # scala\n",
    "# userArtistCSVDF2\n",
    "#     .where($\"playcount\" > 64800)\n",
    "#     .withColumn(\"playyear\", round('playcount * 4 / 60F / 24L / 365F, 3))\n",
    "#     .orderBy('playcount.desc)\n",
    "#     .show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# python\n",
    "userArtistCSVDF3 = userArtistCSVDF2.filter(userArtistCSVDF2.playcount <= 64800)\n",
    "userArtistCSVDF3.persist(StorageLevel.MEMORY_ONLY)\n",
    "print(userArtistCSVDF3.count())\n",
    "userArtistCSVDF3.printSchema()\n",
    "userArtistCSVDF3.show()\n",
    "\n",
    "#scala\n",
    "# val userArtistCSVDF3 = userArtistCSVDF2\n",
    "#     .filter(\"playcount <= 64800\")\n",
    "# \n",
    "# userArtistCSVDF3.persist(org.apache.spark.storage.StorageLevel.MEMORY_ONLY)\n",
    "# println(userArtistCSVDF3.count())\n",
    "# userArtistCSVDF3.printSchema()\n",
    "# userArtistCSVDF3.show()\n",
    "# z.show(userArtistCSVDF3.limit(20))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "userArtistCSVDF3.storageLevel"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(userArtistDS.count() - userArtistCSVDF3.count())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "artist_data.txt (아티스트_이름)\n",
    "- text method 로 txt 읽기\n",
    "- csv method 로 txt 읽기\n",
    "- describe(), summary() 를 통해, 데이터 살펴보기\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#python\n",
    "artistDS = spark.read.text(\"hdfs://localhost:9000/data/audio/profiledata_06-May-2005/artist_data.txt\") # \"hdfs://spark-master-01:9000/data/audio/profiledata_06-May-2005/artist_data.txt\"\n",
    "\n",
    "print(artistDS.count())\n",
    "artistDS.printSchema()\n",
    "artistDS.show(truncate=False)\n",
    "\n",
    "# scala\n",
    "# val artistDS = spark\n",
    "#     .read\n",
    "#     .textFile(\"hdfs://spark-master-01:9000/data/audio/profiledata_06-May-2005/artist_data.txt\")\n",
    "# \n",
    "# println(artistDS.count())\n",
    "# artistDS.printSchema()\n",
    "# artistDS.show(truncate=false)\n",
    "# z.show(artistDS.limit(20))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# python\n",
    "artistDF = spark.read.option(\"sep\", \"\\t\").option(\"inferSchema\", True).csv(\"hdfs://localhost:9000/data/audio/profiledata_06-May-2005/artist_data.txt\").toDF(\"artistid\", \"artistname\") # inferSchema 옵션은, schema를 Spark이 자동으로 알아내는 경우 사용\n",
    "    \n",
    "print(artistDF.count())\n",
    "print(artistDS.count() - artistDF.count())\n",
    "artistDF.printSchema()\n",
    "artistDF.show()\n",
    "\n",
    "# scala\n",
    "# val artistDF = spark\n",
    "#     .read\n",
    "#     .option(\"sep\", \"\\t\")\n",
    "#     .option(\"inferSchema\", true)  //--inferSchema => true....\n",
    "#     .csv(\"hdfs://localhost:9000/data/audio/profiledata_06-May-2005/artist_data.txt\")\n",
    "#     .toDF(\"artistid\", \"artistname\")\n",
    "#     \n",
    "# println(artistDF.count())\n",
    "# println(artistDS.count() - artistDF.count())\n",
    "# artistDF.printSchema()\n",
    "# artistDF.show(false)\n",
    "# z.show(artistDF.limit(20))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "artistDF.summary().show() "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "오류 데이터(artistid toInt 타입변환이 안되는 경우) 확인"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# python\n",
    "@F.udf\n",
    "def str_to_int(s):\n",
    "    try:\n",
    "        return int(s)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "artistDF.withColumn(\"artistid_toInt\",str_to_int(artistDF.artistid)).where(\"artistid_toInt is null\").show()\n",
    "artistDF.withColumn(\"artistid_toInt\",str_to_int(artistDF.artistid)).where(\"artistid_toInt is null\").summary().show()\n",
    "\n",
    "# scala\n",
    "# artistDF.filter(row => {\n",
    "#     val artistid = row.getString(0)\n",
    "#     val artistname = row.getString(1)\n",
    "#     try {\n",
    "#         artistid.toInt\n",
    "#         false\n",
    "#     } catch {\n",
    "#         case e:Exception => true  //--int로 형변환이 되지 않는 것만 필터링해서 살펴보자....\n",
    "#     }\n",
    "#     \n",
    "# }).show(false)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "오류 데이터 제거"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# python\n",
    "artistFinal = artistDF.withColumn(\"artistid\",str_to_int(artistDF.artistid)).where(\"artistid is not null\").where('artistname is not null').selectExpr(\"cast(artistid as int) artistid\", 'artistname')\n",
    "print(artistFinal.count())\n",
    "artistFinal.printSchema()\n",
    "artistFinal.show()\n",
    "\n",
    "# scala\n",
    "# val artistFinal = artistDF.filter(row => {\n",
    "#     val artistid = row.getString(0)\n",
    "#     val artistname = row.getString(1)\n",
    "#     try {\n",
    "#         artistid.toInt\n",
    "#         true\n",
    "#     } catch {\n",
    "#         case e:Exception => false  //--int로 형변환이 되지 않는 것은 버리자....\n",
    "#     }\n",
    "# })\n",
    "# .where(\"artistid is not null\")\n",
    "# .where(\"artistname is not null\")\n",
    "# .withColumn(\"artistid\", expr(\"cast(artistid as int)\"))\n",
    "# \n",
    "# println(artistFinal.count())\n",
    "# artistFinal.printSchema()\n",
    "# artistFinal.show(false)\n",
    "# z.show(artistFinal.limit(20))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "artistFinal.summary().show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 아티스트 이름이 null인 데이터 확인\n",
    "artistFinal.where(\"artistname is null\").show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#아티스트 이름이 이상한(숫자) 데이터 확인\n",
    "artistFinal.where(\"artistname in ('33', '304', '1988')\").show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#아티스트 이름이 이상한(숫자) 데이터 확인\n",
    "artistDS.where(\"value like '1335772%' or value like '1344623%' or value like '2032179%'\").show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#아티스트 이름이 이상한(min/max) 데이터 확인\n",
    "artistFinal.where(\"artistname in ('\u0001', '￿￿￿￿￿￿￿￿￿￿￿￿くȁ')\").show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "artistFinal.where(\"artistname in (' ', '￿￿￿￿￿￿￿￿￿￿￿￿くȁ')\").show()\n",
    "artistFinal.where(\"artistname in ('.', '￿￿￿￿￿￿￿￿￿￿￿￿くȁ')\").show()\n",
    "artistFinal.where(\"artistname in ('\u0001', '￿￿￿￿￿￿￿￿￿￿￿￿くȁ')\").show()\n",
    "\n",
    "artistDS.where(\"value like '6986651%' or value like '9915481%' or value like '1025136%' or value like '1165062%'\").show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "artistDS.where(\"value like '1165062%' or value like '10495051%'\").show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 걸러진 데이터 건수 확인\n",
    "print(artistDS.count() - artistFinal.count())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### (3) artist_alias.txt (배드아이디_굿아이디)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# python\n",
    "artistAliasDS = spark.read.text(\"hdfs://localhost:9000/data/audio/profiledata_06-May-2005/artist_alias.txt\") # hdfs://spark-master-01:9000/data/audio/profiledata_06-May-2005/artist_alias.txt\n",
    "    \n",
    "print(artistAliasDS.count())\n",
    "artistAliasDS.printSchema()\n",
    "artistAliasDS.show()\n",
    "\n",
    "#scala\n",
    "# val artistAliasDS = spark\n",
    "#     .read\n",
    "#     .textFile(\"hdfs://spark-master-01:9000/data/audio/profiledata_06-May-2005/artist_alias.txt\")\n",
    "#     \n",
    "# println(artistAliasDS.count())\n",
    "# artistAliasDS.printSchema()\n",
    "# artistAliasDS.show()\n",
    "# z.show(artistAliasDS.limit(20))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# python\n",
    "artistAliasDF = spark.read.option(\"sep\", \"\\t\").option(\"inferSchema\", True).csv(\"hdfs://localhost:9000/data/audio/profiledata_06-May-2005/artist_alias.txt\").toDF(\"badid\", \"goodid\") # hdfs://spark-master-01:9000/data/audio/profiledata_06-May-2005/artist_alias.txt\n",
    "print(artistAliasDF.count())\n",
    "artistAliasDF.printSchema()\n",
    "artistAliasDF.show()\n",
    "\n",
    "# scala\n",
    "# val artistAliasDF = spark\n",
    "#     .read\n",
    "#     .option(\"sep\", \"\\t\")\n",
    "#     .option(\"inferSchema\", true)  //--inferSchema => true....\n",
    "#     .csv(\"hdfs://spark-master-01:9000/data/audio/profiledata_06-May-2005/artist_alias.txt\")\n",
    "#     .toDF(\"badid\", \"goodid\")\n",
    "#     \n",
    "# println(artistAliasDF.count())\n",
    "# artistAliasDF.printSchema()\n",
    "# artistAliasDF.show()\n",
    "# z.show(artistAliasDF.limit(20))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 통계정보 확인\n",
    "artistAliasDF.summary().show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "summary() 의 count 정보를 확인했을때, badid 와 goodid 의 개수가 일치하지 않음"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "artistAliasDF.where(\"badid is null\").show(truncate=False)\n",
    "artistAliasDF.filter(\"goodid is null\").show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "데이터에서 Null 제거"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# python\n",
    "artistAliasFinal = artistAliasDF.filter(\"badid is not null\").filter(\"goodid is not null\")\n",
    "\n",
    "print(artistAliasFinal.count())\n",
    "artistAliasFinal.printSchema()\n",
    "artistAliasFinal.show()\n",
    "\n",
    "artistAliasFinal.createOrReplaceTempView(\"artistAliasFinal\")\n",
    "\n",
    "# scala\n",
    "# val artistAliasFinal = artistAliasDF\n",
    "#     .filter(\"badid is not null\")\n",
    "#     .filter(\"goodid is not null\")\n",
    "# \n",
    "# println(artistAliasFinal.count())\n",
    "# artistAliasFinal.printSchema()\n",
    "# artistAliasFinal.show()\n",
    "# z.show(artistAliasFinal.limit(20))\n",
    "# \n",
    "# artistAliasFinal.createOrReplaceTempView(\"artistAliasFinal\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "artistAliasFinal.summary().show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 걸러진 데이터 건수 확인\n",
    "print(artistAliasDS.count() - artistAliasFinal.count())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "(4) user_artist_data.txt(유저_아티스트_플레이카운트 데이터) 에서 아티스트의 BadID를 GoodID로 변경하는 작업수행\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TempView 등록\n",
    "userArtistCSVDF3.printSchema()\n",
    "artistAliasFinal.printSchema()\n",
    "\n",
    "userArtistCSVDF3.createOrReplaceTempView(\"userArtistCSVDF3\")\n",
    "artistAliasFinal.createOrReplaceTempView(\"artistAliasFinal\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# badid 목록 보기\n",
    "%%sparksql sql_df\n",
    "\n",
    "SELECT\n",
    "    distinct(badid)\n",
    "FROM\n",
    "    artistAliasFinal\n",
    "limit 20;"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sql_df.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# badid 개수 및 목록 보기\n",
    "\n",
    "# artistAliasFinal.select(distinct($\"badid\")).show() \n",
    "# functions에 distinct 함수 자체가 아직 없어요.... error: not found: value distinct\n",
    "artistAliasFinal.select(F.count_distinct(artistAliasFinal.badid)).show() #--functions에 countDistinct 함수 존재함.... 정상동작....\n",
    "\n",
    "#artistAliasFinal.selectExpr(\"distinct(badid)\").show()  //--distinct(col) 에러발생.... AnalysisException: Undefined function: 'distinct'....\n",
    "artistAliasFinal.selectExpr(\"count(distinct(badid))\").show()  #count(distinct(col)) 정상동작....\n",
    "\n",
    "\n",
    "spark.sql(\"select distinct(badid) from artistAliasFinal\").show()  #distinct(col) 정상동작....\n",
    "spark.sql(\"select count(distinct(badid)) from artistAliasFinal\").show()  #count(distinct(col)) 정상동작...."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# badid 갯수 확인\n",
    "print(artistAliasFinal.count())\n",
    "artistAliasFinal.selectExpr(\"count(distinct(badid))\").show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "badid를 goodid로 변경하기 위한 join 테스트"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%sparksql badTogood_df_1\n",
    "\n",
    "select\n",
    "    *\n",
    "from\n",
    "    userArtistCSVDF3 ua\n",
    "    left outer join\n",
    "    artistAliasFinal aa\n",
    "    on \n",
    "    ua.artistid = aa.badid\n",
    "where\n",
    "    True\n",
    "limit 20;"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "badTogood_df_1.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%sparksql badTogood_df_2\n",
    "\n",
    "select \n",
    "    ua.*,\n",
    "    aa.*,\n",
    "    case\n",
    "        when aa.badid is not null then aa.goodid\n",
    "        else ua.artistid\n",
    "    end\n",
    "    as artistid2\n",
    "from\n",
    "    userArtistCSVDF3 ua\n",
    "    left outer join\n",
    "    artistAliasFinal aa\n",
    "    on \n",
    "    ua.artistid = aa.badid\n",
    "where\n",
    "    true\n",
    "limit 20;"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "badTogood_df_2.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%sparksql badTogood_df_3\n",
    "\n",
    "select \n",
    "    ua.userid,\n",
    "    case\n",
    "        when aa.badid is not null then aa.goodid\n",
    "        else ua.artistid\n",
    "    end\n",
    "    as artistid,\n",
    "    ua.playcount\n",
    "from\n",
    "    userArtistCSVDF3 ua\n",
    "    left outer join\n",
    "    artistAliasFinal aa\n",
    "    on \n",
    "    ua.artistid = aa.badid\n",
    "where\n",
    "    true\n",
    "--and aa.badid is not null\n",
    "limit 20;\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "badTogood_df_3.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# python\n",
    "userArtistCSVDF4 = spark.sql('select ua.userid, case when aa.badid is not null then aa.goodid else ua.artistid end as artistid, ua.playcount from userArtistCSVDF3 ua left outer join artistAliasFinal aa on ua.artistid = aa.badid where true')\n",
    "\n",
    "print(userArtistCSVDF4.count())\n",
    "userArtistCSVDF4.printSchema()\n",
    "userArtistCSVDF4.show()\n",
    "\n",
    "# scala\n",
    "# val userArtistCSVDF4 = spark.sql(\"\"\"\n",
    "# \n",
    "# select \n",
    "#     ua.userid,\n",
    "#     case\n",
    "#         when aa.badid is not null then aa.goodid\n",
    "#         else ua.artistid\n",
    "#     end\n",
    "#     as artistid,\n",
    "#     ua.playcount\n",
    "# from\n",
    "#     userArtistCSVDF3 ua\n",
    "#     left outer join\n",
    "#     artistAliasFinal aa\n",
    "#     on \n",
    "#     ua.artistid = aa.badid\n",
    "# where\n",
    "#     true\n",
    "# --and aa.badid is not null\n",
    "# \"\"\")\n",
    "# \n",
    "# println(userArtistCSVDF4.count())\n",
    "# userArtistCSVDF4.printSchema()\n",
    "# userArtistCSVDF4.show()\n",
    "# z.show(userArtistCSVDF4.limit(20))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "SQL문으로 작업한 내용을 DataFrame의 API로 작업하기 => badid를 goodid로 바꾼 최종 데이터"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# python\n",
    "userArtistFinal = userArtistCSVDF3.join(artistAliasFinal, userArtistCSVDF3.artistid== artistAliasFinal.badid, \"left_outer\").withColumn(\"artistid2\", F.expr(\"case when badid is not null then goodid else artistid end\")).withColumn(\"artistid2\", F.when(F.col(\"badid\").isNotNull(), F.col(\"goodid\")).otherwise(F.col(\"artistid\"))).select('userid', F.col('artistid2').alias(\"artistid\"), 'playcount')\n",
    "# userArtistFinal = spark.sql('select * from userArtistCSVDF3 as ua left outer join artistAliasFinal as aa on ua.artistid = aa.badid').withColumn(\"artistid2\", F.expr(\"case when badid is not null then goodid else artistid end\")).withColumn(\"artistid2\", F.when(F.col(\"aa.badid\").isNotNull(), F.col(\"aa.goodid\")).otherwise(F.col(\"ua.artistid\"))).select('userid', F.col('artistid2').alias('artistid'), 'playcount')\n",
    "\n",
    "userArtistFinal.persist(StorageLevel.MEMORY_ONLY)    \n",
    "print(userArtistFinal.count())\n",
    "userArtistFinal.printSchema()\n",
    "userArtistFinal.show()\n",
    "\n",
    "userArtistFinal.createOrReplaceTempView(\"userArtistFinal\")\n",
    "\n",
    "# scala\n",
    "# val userArtistFinal = userArtistCSVDF3.as(\"ua\")\n",
    "#     .join(artistAliasFinal.as(\"aa\"), $\"ua.artistid\" === $\"aa.badid\", \"left_outer\")\n",
    "# //  .withColumn(\"artistid2\", case when $\"aa.badid\" is not null then $\"aa.goodid\" else $\"ua.artistid\" end)  //--case 함수 제공 안함....\n",
    "#     .withColumn(\"artistid2\", expr(\"case when aa.badid is not null then aa.goodid else ua.artistid end\"))  //--expr 함수 사용....\n",
    "#     .withColumn(\"artistid2\", when(col(\"aa.badid\").isNotNull, col(\"aa.goodid\")).otherwise(col(\"ua.artistid\")))  //--when 함수 사용....\n",
    "#     .select('userid, 'artistid2 as \"artistid\", 'playcount)\n",
    "# \n",
    "# userArtistFinal.persist(org.apache.spark.storage.StorageLevel.MEMORY_ONLY)    \n",
    "# println(userArtistFinal.count())\n",
    "# userArtistFinal.printSchema()\n",
    "# userArtistFinal.show()\n",
    "# z.show(userArtistFinal.limit(20))\n",
    "# \n",
    "# userArtistFinal.createOrReplaceTempView(\"userArtistFinal\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 걸러진 데이터 건수 확인\n",
    "print(userArtistCSVDF3.count() - userArtistFinal.count())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 걸러진 badid 데이터 건수 확인\n",
    "print(userArtistCSVDF3.select(\"artistid\").distinct().count() - userArtistFinal.select(\"artistid\").distinct().count())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "badid를 goodid로 바꾼 후 같은 아티스트에 대한 playcount가 2개 이상인 데이터 확인 #1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%sparksql plc_check_1\n",
    "\n",
    "select \n",
    "    userid,\n",
    "    artistid,\n",
    "    count(playcount) as cnt\n",
    "from\n",
    "    userArtistFinal\n",
    "group by userid, artistid\n",
    "having\n",
    "    True\n",
    "and cnt > 1\n",
    "order by cnt DESC\n",
    "limit 20;"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%sparksql plc_check_2\n",
    "\n",
    "select \n",
    "    *\n",
    "from\n",
    "    userArtistFinal\n",
    "where\n",
    "    True\n",
    "and userid = 2133748\n",
    "and artistid = 1018110\n",
    ";\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%sparksql plc_check_3\n",
    "\n",
    "select\n",
    "    *\n",
    "from \n",
    "    artistAliasFinal\n",
    "where\n",
    "    True\n",
    "and goodid = 1018110\n",
    ";\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plc_check_3.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "같은 아티스트에 대한 plyacount 합치기"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%sparksql plc_check_4\n",
    "\n",
    "select \n",
    "    userid,\n",
    "    artistid,\n",
    "    sum(playcount) as playcount\n",
    "from\n",
    "    userArtistFinal\n",
    "group by userid, artistid\n",
    "order by playcount desc\n",
    "limit 20;"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "같은 아티스트에 대한 playcount 합치기 ( DF API로 작업하기 )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# python\n",
    "userArtistFinal2 = userArtistFinal.groupBy(\"userid\", \"artistid\").agg(F.sum('playcount').alias('playcount')).filter('playcount <= 64800') # 이전에 설정한 max playcount 값인 '64800'로 데이터 걸러내기\n",
    "\n",
    "userArtistFinal2.printSchema()\n",
    "userArtistFinal2.show()\n",
    "\n",
    "# scala\n",
    "# val userArtistFinal2 = userArtistFinal\n",
    "#     .groupBy(\"userid\", \"artistid\")\n",
    "#     .agg(sum(\"playcount\").as(\"playcount\"))\n",
    "#     .filter(\"playcount <= 64800\")  //--저희의 max playcount 값 '64800'로 데이터 걸러내기....\n",
    "# \n",
    "# userArtistFinal2.persist(org.apache.spark.storage.StorageLevel.MEMORY_ONLY)    \n",
    "# println(userArtistFinal2.count())\n",
    "# userArtistFinal2.printSchema()\n",
    "# userArtistFinal2.show()\n",
    "# z.show(userArtistFinal2.limit(20))\n",
    "# \n",
    "# userArtistFinal2.createOrReplaceTempView(\"userArtistFinal2\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#  playcount가 합쳐진 데이터 건수 확인\n",
    "print(userArtistFinal.count() - userArtistFinal2.count())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 최종 학습 데이터 캐시\n",
    "\n",
    "userArtistFinal2.cache() # userid_artistid_playcount\n",
    "\n",
    "userArtistFinal2.persist(StorageLevel.MEMORY_ONLY) \n",
    "userArtistFinal2.createOrReplaceTempView(\"userArtistFinal2\")\n",
    "print(userArtistFinal2.count())\n",
    "\n",
    "# 기타 데이터 캐시\n",
    "artistFinal.cache()  # artistid_artistname\n",
    "print(artistFinal.count())\n",
    "artistAliasFinal.persist()  # badid_goodid\n",
    "print(artistAliasFinal.count())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 최종 학습 데이터 캐시 해제\n",
    "userArtistFinal2.unpersist()  #userid_artistid_playcount\n",
    "\n",
    "# 기타 데이터 캐시 해제\n",
    "artistFinal.unpersist()  #artistid_artistname\n",
    "artistAliasFinal.unpersist()  #badid_goodid"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cassandra 에 userArtistFinal2 저장하기 **\n",
    "- apache-cassandra-4.0.11/bin/cassandra\n",
    "- Table 생성"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# cassandra 와 연결\n",
    "# 참고사이트 : https://docs.datastax.com/en/developer/python-driver/3.24/getting_started/\n",
    "\n",
    "try:\n",
    "    cluster = Cluster(['127.0.0.1'], port=9042)\n",
    "    session = cluster.connect(keyspace='mykeyspace')\n",
    "except Exception as e:\n",
    "    print(e)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "session.execute('describe keyspaces').all()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# [Cassandra] Table List 조회\n",
    "session.execute('describe tables').all()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# [Cassandra] userArtistFinal2 Table 내용 확인\n",
    "session.execute('select * from user_artist_data limit 20').all()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# [Cassandra] Table 스키마 확인\n",
    "\n",
    "session.execute(\"SELECT column_name,type FROM system_schema.columns WHERE keyspace_name ='mykeyspace' and table_name='user_artist_data'\").all()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "userArtistFinal2.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Cassandra] 최종 학습 데이터 저장"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Table 생성\n",
    "session.execute(\"CREATE TABLE user_artist_data2 ( userid int , artistid int , playcount int, primary key (userid, artistid))\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Table 에 data insert 하기\n",
    "insert_query = session.prepare(\"INSERT INTO user_artist_data2 (userid, artistid, playcount) VALUES (?, ?, ?)\")\n",
    "parameters = [(userid, artistid, playcount) for userid, artistid, playcount in userArtistFinal2.limit(100000).toLocalIterator()] # 시간 단축을 위해, 데이터 일부만 저장\n",
    "execute_concurrent_with_args(session, insert_query, parameters, concurrency=50)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. ALS 알고리즘 학습\n",
    "\n",
    "\n",
    "```\n",
    "추천 알고리즘 인 ALS를 이용하여 모델 생성 및 추천 실행....\n",
    " \n",
    "ALS(\"Alternating Least Squares\", \"교차 최소 제곱\") 알고리즘은 Netflix Prize에서 \n",
    "발표된 논문인 \"Collaborative Filtering for the Implicit Feedback Datasets\"과, \n",
    "\"Large-scale Parallel Collaborative Filtering for the Netflix Prize\"에서 주로 사용된 방식....\n",
    "\n",
    "Spark MLlib의 ALS는 이 두 논문에서 아이디어를 가져와 구현....\n",
    "\n",
    "- Collaborative Filtering for Implicit Feedback Datasets\n",
    "(http://yifanhu.net/PUB/cf.pdf)\n",
    "\n",
    "Large-scale Parallel Collaborative Filtering for the Netflix Prize\n",
    "(http://shiftleft.com/mirrors/www.hpl.hp.com/personal/Robert_Schreiber/papers/2008%20AAIM%20Netflix/netflix_aaim08(submitted).pdf)\n",
    "```\n",
    "\n",
    "#### [Matrix Completion]\n",
    "\n",
    "![Matrix Completion](https://d3i71xaburhd42.cloudfront.net/07d2577de9fb4bb5cbd7424ce5d64e6ef0dd78a0/30-Figure2.1-1.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### [ALS(\"Alternating Least Squares\", \"교차 최소 제곱\")]\n",
    "\n",
    "![ALS](https://miro.medium.com/max/1400/1*ezY_g30VQ8MTGpDwd3z56w.png)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Cassandra] 최종 학습 데이터 로딩 + Cache"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "userArtistFinal3 = spark.createDataFrame(data=session.execute(\"SELECT * FROM user_artist_data\").all(), schema = ['userid', 'artistid', 'playcount'])\n",
    "#최종 학습 데이터 캐시\n",
    "userArtistFinal3.persist(StorageLevel.MEMORY_ONLY) #userid_artistid_playcount\n",
    "print(userArtistFinal3.count())\n",
    "\n",
    "userArtistFinal3.printSchema()\n",
    "userArtistFinal3.show()\n",
    "\n",
    "# scala\n",
    "# val userArtistFinal3 = spark.table(\"mycatalog.mykeyspace.user_artist_data\")\n",
    "# \n",
    "# //--최종 학습 데이터 캐시....\n",
    "# userArtistFinal3.persist(org.apache.spark.storage.StorageLevel.MEMORY_ONLY) //--userid_artistid_playcount....\n",
    "# println(userArtistFinal3.count())\n",
    "# \n",
    "# userArtistFinal3.printSchema\n",
    "# userArtistFinal3.show"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "(option) [캐시해제] 최종 학습 데이터 캐시 해제"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 최종 학습 데이터 캐시 해제\n",
    "userArtistFinal3.unpersist()  # userid_artistid_playcount"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "ALS 알고리즘 파라미터 세팅"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# python\n",
    "als = ALS(rank=10,coldStartStrategy='drop',seed=11, alpha=40, regParam=0.1, maxIter=5, implicitPrefs=True, ratingCol='playcount', itemCol='artistid', userCol='userid') # als = ALS().setUserCol(\"userid\").setItemCol(\"artistid\").setRatingCol(\"playcount\").setImplicitPrefs(True).setMaxIter(5).setRegParam(0.1).setAlpha(40).setRank(10).setColdStartStrategy(\"drop\").setSeed(11)\n",
    "\n",
    "# scala\n",
    "# val als = new ALS()\n",
    "#     .setUserCol(\"userid\")\n",
    "#     .setItemCol(\"artistid\")\n",
    "#     .setRatingCol(\"playcount\")\n",
    "#     .setImplicitPrefs(true)\n",
    "#     .setMaxIter(5)\n",
    "#     .setRegParam(0.1)  //--hyper parameter.... Param for regularization parameter (>= 0).\n",
    "#     .setAlpha(40)  //--hyper parameter.... Param for the alpha parameter in the implicit preference formulation (nonnegative). Default: 1.0\n",
    "#     .setRank(10)  //--hyper parameter.... Param for rank of the matrix factorization (positive). Default: 10\n",
    "#     .setColdStartStrategy(\"drop\")  //--Param for strategy for dealing with unknown or new users/items at prediction time. Supported values: nan,drop. (default: nan)\n",
    "#     .setSeed(11L)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 파라미터 내용 보기\n",
    "print(\"\\n>>>> als.explainParams()\")\n",
    "print(als.explainParams())\n",
    "\n",
    "print(\"\\n\\n>>>> als.extractParamMap()\")\n",
    "als.extractParamMap()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "userArtistFinal3.printSchema()\n",
    "\n",
    "# ALS 알고리즘 학습\n",
    "alsModel = als.fit(userArtistFinal3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. 추천\n",
    "\n",
    "- 존재하는 사용자 vs. 존재하지 않는(New) 사용자\n",
    "- 모든 사용자 : 아티스트 쌍에 대해 모델 적용\n",
    "- 모든 사용자에 대한 추천\n",
    "- 모든 아티스트에 대한 추천\n",
    "- 아티스트 이름도 같이 보기\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "테스트용 사용자 추출"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# python\n",
    "userArtistFinal3.groupBy(\"userid\").agg(F.count(\"artistid\").alias(\"count_artist\"), F.sum(\"playcount\").alias(\"sum_playcount\")).where(\"count_artist >= 20\").orderBy(F.col(\"count_artist\").asc(), F.col(\"sum_playcount\").desc()).show()\n",
    "\n",
    "# scala\n",
    "# userArtistFinal3\n",
    "#     .groupBy(\"userid\")\n",
    "#     .agg(count(\"artistid\").as(\"count_artist\"), sum(\"playcount\").as(\"sum_playcount\"))\n",
    "#     .where(\"count_artist >= 20\")\n",
    "#     .orderBy($\"count_artist\".asc, $\"sum_playcount\".desc)\n",
    "#     .show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "학습된 ALS Model로 추천해보기 > Exist User & New User (w/ 아티스트 이름....)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# python\n",
    "userDS =spark.createDataFrame([1001440, 2010008, 987654321], IntegerType()).toDF('userID')\n",
    "userDS.printSchema()\n",
    "userDS.show()\n",
    "\n",
    "# 특정 사용자를 위한 추천 5개....\n",
    "recommendedForSomeUsersDF = alsModel.recommendForUserSubset(userDS, 5)\n",
    "recommendedForSomeUsersDF.printSchema()\n",
    "recommendedForSomeUsersDF.show()\n",
    "\n",
    "# explode....\n",
    "recommendedForSomeUsersDF2 = recommendedForSomeUsersDF.withColumn(\"recommend\", F.explode(\"recommendations\")).withColumn(\"artistid\", F.col('recommend').artistid).withColumn(\"rating\",  F.col(\"recommend\").rating)\n",
    "\n",
    "recommendedForSomeUsersDF2.printSchema()\n",
    "recommendedForSomeUsersDF2.show()\n",
    "print(recommendedForSomeUsersDF2.count())\n",
    "\n",
    "# 불필요한 colum drop하기\n",
    "recommendedForSomeUsersDF3 = recommendedForSomeUsersDF2.drop(\"recommendations\", \"recommend\")\n",
    "\n",
    "recommendedForSomeUsersDF3.printSchema()\n",
    "recommendedForSomeUsersDF3.show()\n",
    "print(recommendedForSomeUsersDF3.count())\n",
    "\n",
    "# recommendedForSomeUsersDF3 와 artistFinal DF 를 join 하여, artistid 로 artis name 가져오기\n",
    "recommendedForSomeUsersDF4 = recommendedForSomeUsersDF3.join(artistFinal, 'artistid').orderBy(F.col(\"userid\").asc(), F.col(\"rating\").desc())\n",
    "recommendedForSomeUsersDF4.show()\n",
    "\n",
    "# scala\n",
    "# val userDS = Seq(1001440, 2010008, 987654321)\n",
    "#     .toDF(\"userID\")\n",
    "#     .as[Int]  //--Dataset으로 형변환.... by Encoder....\n",
    "# \n",
    "# userDS.printSchema\n",
    "# userDS.show(false)\n",
    "# \n",
    "# //--특정 사용자를 위한 추천 5개....\n",
    "# val recommendedForSomeUsersDF = alsModel.recommendForUserSubset(userDS, 5)\n",
    "# recommendedForSomeUsersDF.printSchema\n",
    "# recommendedForSomeUsersDF.show(false)\n",
    "# \n",
    "# //--explode....\n",
    "# val recommendedForSomeUsersDF2 = recommendedForSomeUsersDF\n",
    "#     .withColumn(\"recommend\", explode($\"recommendations\"))\n",
    "#     .withColumn(\"artistid\", $\"recommend.artistid\")\n",
    "#     .withColumn(\"rating\", $\"recommend.rating\")\n",
    "# \n",
    "# recommendedForSomeUsersDF2.printSchema()\n",
    "# recommendedForSomeUsersDF2.show(false)\n",
    "# println(recommendedForSomeUsersDF2.count())\n",
    "# \n",
    "# //--drop....\n",
    "# val recommendedForSomeUsersDF3 = recommendedForSomeUsersDF2\n",
    "#     .drop(\"recommendations\", \"recommend\")\n",
    "# \n",
    "# recommendedForSomeUsersDF3.printSchema()\n",
    "# recommendedForSomeUsersDF3.show(false)\n",
    "# println(recommendedForSomeUsersDF3.count())\n",
    "# \n",
    "# //--join w/artistFinal....\n",
    "# val recommendedForSomeUsersDF4 = recommendedForSomeUsersDF3.as(\"reco\")\n",
    "#     .join(artistFinal.as(\"art\"), $\"reco.artistid\" === $\"art.artistid\")\n",
    "#     .orderBy($\"userid\".asc, $\"rating\".desc)\n",
    "# \n",
    "# recommendedForSomeUsersDF4.show(false)\n",
    "# z.show(recommendedForSomeUsersDF4)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "그럴싸한 추천을 제공하는지 확인해보기 > 기존 플레이한 아티스트 vs. 추천된 아티스트 비교"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "userArtistFinal3.createOrReplaceTempView(\"userArtistFinal3\");\n",
    "artistFinal.createOrReplaceTempView(\"artistFinal\");"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# python\n",
    "historyForSomeUsersDF = spark.sql('select * from (select * from userArtistFinal3 where userid in (1001440, 2010008)) as history join artistFinal as art on history.artistid = art.artistid').orderBy(F.col(\"userid\").asc(), F.col(\"playcount\").desc())\n",
    "\n",
    "historyForSomeUsersDF.show(40, False)\n",
    "\n",
    "# scala\n",
    "# val historyForSomeUsersDF = userArtistFinal3\n",
    "#     .where(\"userid in (1001440, 2010008)\").as(\"history\")\n",
    "#     .join(artistFinal.as(\"art\"), $\"history.artistid\" === $\"art.artistid\")\n",
    "#     .orderBy($\"userid\".asc, $\"playcount\".desc)\n",
    "# \n",
    "# historyForSomeUsersDF.show(40, false)\n",
    "# z.show(historyForSomeUsersDF)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "모든 (사용자 : 아티스트) 쌍에 대해 모델 적용하기"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# python\n",
    "predictionsDF = alsModel.transform(userArtistFinal3)\n",
    "predictionsDF.printSchema()\n",
    "\n",
    "predictionsDF.orderBy(F.col(\"prediction\").desc()).show()\n",
    "predictionsDF.orderBy(F.col(\"prediction\").asc()).show()\n",
    "\n",
    "# scala\n",
    "# val predictionsDF = alsModel.transform(userArtistFinal3)\n",
    "# predictionsDF.printSchema\n",
    "# \n",
    "# predictionsDF\n",
    "#     .orderBy($\"prediction\".desc)\n",
    "#     .show(false)\n",
    "# \n",
    "# predictionsDF\n",
    "#     .orderBy($\"prediction\".asc)\n",
    "#     .show(false)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5. 모델 평가\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "모델 평가를 위한 Evaluator 정의 (평가지표 : RMSE)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# python\n",
    "regEval = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"playcount\", metricName=\"rmse\" )\n",
    "\n",
    "# scala\n",
    "# val regEval = new RegressionEvaluator()\n",
    "#     .setLabelCol(\"playcount\")\n",
    "#     .setPredictionCol(\"prediction\")\n",
    "#     .setMetricName(\"rmse\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "모델 평가"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rmse = regEval.evaluate(predictionsDF)\n",
    "\n",
    "print(rmse)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6. 하이퍼 파라미터 튜닝"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pipeline 알고리즘 정의 > stage 1개 + ALS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# python\n",
    "pipeline = Pipeline(stages=[als]) # 다른 방법 pipeline = Pipeline.setStages(als)\n",
    "\n",
    "# scala\n",
    "# val pipeline = new Pipeline()\n",
    "#     .setStages(Array(als))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "하이퍼 파라미터 조합을 ParamMap으로 구성하기"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# python\n",
    "paramMaps = ParamGridBuilder().addGrid(als.alpha, (40.0, 10.0, 5.0, 1.0)).addGrid(als.rank, (2,3,10)).addGrid(als.regParam, (1.0, 0.01)).build()\n",
    "\n",
    "print(paramMaps)\n",
    "\n",
    "# scala\n",
    "# val paramMaps = new ParamGridBuilder()\n",
    "#     .addGrid(als.alpha, Array(40.0,10.0, 5.0, 1.0))\n",
    "#     .addGrid(als.rank, Array(2, 3, 10))\n",
    "#     .addGrid(als.regParam, Array(1.0, 0.01))\n",
    "#     .build()\n",
    "# \n",
    "# println(paramMaps.mkString(\", \"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "CrossValidator 정의"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# python\n",
    "\"\"\"\n",
    "estimator : ML 알고리즘\n",
    "estimatorParamMaps : 튜닝하고자 하는 하이퍼 파라미터 조합\n",
    "evaluator : 모델을 평가하는 평가자\n",
    "numFolds : 데이터를 나누는 기준\n",
    "\"\"\"\n",
    "cv = CrossValidator(estimator=pipeline,estimatorParamMaps=paramMaps,evaluator=regEval,numFolds=2 )  \n",
    "\n",
    "# scala\n",
    "# val cv = new CrossValidator()\n",
    "#     .setEstimator(pipeline)  //--ML 알고리즘.... Pipeline....\n",
    "#     .setEstimatorParamMaps(paramMaps)  //--튜닝하고자 하는 하이퍼 파라미터 조합....\n",
    "#     .setEvaluator(regEval)  //--모델을 평가하는 평가자....\n",
    "#     .setNumFolds(2)  //--데이터를 나누는 기준....\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "train dataset 과 test dataset 분리하기"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# python\n",
    "(trainDS, testDS) = userArtistFinal3.randomSplit([0.7, 0.3], 11)\n",
    "print(userArtistFinal3.count())\n",
    "\n",
    "trainDS.persist(StorageLevel.MEMORY_ONLY)\n",
    "print(trainDS.count())\n",
    "\n",
    "testDS.persist(StorageLevel.MEMORY_ONLY)\n",
    "print(testDS.count())\n",
    "\n",
    "# scala\n",
    "# val Array(trainDS, testDS) = userArtistFinal3.randomSplit(Array(0.7, 0.3), 11L)\n",
    "# \n",
    "# println(userArtistFinal3.count)\n",
    "# \n",
    "# trainDS.persist(org.apache.spark.storage.StorageLevel.MEMORY_ONLY)\n",
    "# println(trainDS.count)\n",
    "# \n",
    "# testDS.persist(org.apache.spark.storage.StorageLevel.MEMORY_ONLY)\n",
    "# println(testDS.count)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "CrossValidator 학습 > Took 1 min 50 sec"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cvModel = cv.fit(trainDS)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Best 모델 조회"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# python\n",
    "print(\"\\n>>>> Best Model : \\n\" + str(cvModel.bestModel))\n",
    "print(\"\\n>>>> Avg Metrics :\", *cvModel.avgMetrics, sep='\\n' )\n",
    "print(\"\\n>>>> Estimator ParamMaps :\", *cvModel.getEstimatorParamMaps(), sep='\\n')\n",
    "\n",
    "# scala\n",
    "# print(\"\\n>>>> Best Model : \\n\" + cvModel.bestModel)\n",
    "# print(\"\\n>>>> Avg Metrics : \\n\" + cvModel.avgMetrics.mkString(\"\\n\"))\n",
    "# print(\"\\n>>>> Estimator ParamMaps : \\n\" + cvModel.getEstimatorParamMaps.mkString(\"\\n\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "파라미터 조합별 metric 점수 매핑하여 보기"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# python\n",
    "zippedParamAndMetrics = zip(cvModel.getEstimatorParamMaps(),cvModel.avgMetrics)\n",
    "print(*(\"\\n>>>> Zipped Param And Metrics : \\n\" , *zippedParamAndMetrics ),sep='\\n')\n",
    "\n",
    "# scala\n",
    "# val zippedParamAndMetrics = cvModel.getEstimatorParamMaps\n",
    "#     .zip(cvModel.avgMetrics)\n",
    "#     .sortBy(_._2)\n",
    "# \n",
    "# println(\"\\n>>>> Zipped Param And Metrics : \\n\" + zippedParamAndMetrics.mkString(\"\\n\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "모델 평가 (RMSE....) #2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# python\n",
    "predictionsDF2 = cvModel.transform(testDS)\n",
    "predictionsDF2.show()\n",
    "\n",
    "rmse2 = regEval.evaluate(predictionsDF2)\n",
    "print(rmse2)\n",
    "\n",
    "# scala\n",
    "# val predictionsDF2 = cvModel.transform(testDS)\n",
    "# predictionsDF2.show()\n",
    "# \n",
    "# val rmse2 = regEval.evaluate(predictionsDF2)\n",
    "# println(rmse2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "CrossValidator 모델 HDFS에 저장"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# python\n",
    "cvModel.write().overwrite().save(\"hdfs://localhost:9000/model/audio/profiledata_06-May-2005/als_crossvalidator\")\n",
    "\n",
    "# scala\n",
    "# cvModel.write.overwrite.save(\"hdfs://spark-master-01:9000/model/audio/profiledata_06-May-2005/als_crossvalidator\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "실제 Best 모델인 ALSModel 추출하여 HDFS에 저장"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# python\n",
    "print(f\">>>> CrossValidatorModel : {cvModel} \\n\")\n",
    "print(f\">>>> CrossValidatorModel.bestModel : {cvModel.bestModel}\\n\")\n",
    "print(f\">>>> PipelineModel.stages(0) : {cvModel.bestModel.stages[0]}\\n\")\n",
    "\n",
    "# ALSModel\n",
    "alsBestModel = cvModel.bestModel.stages[0]\n",
    "\n",
    "# 실제 Best 모델인 ALSModel을 HDFS에 저장 -> 왜? ALSModel에 있는 recommendForUserSubset() API와 같은 추천에 특화된 API를 사용하려구\n",
    "alsBestModel.write().overwrite().save(\"hdfs://localhost:9000/model/audio/profiledata_06-May-2005/als\")\n",
    "\n",
    "# scala\n",
    "# println(\"\\n>>>> CrossValidatorModel : \\n\" + cvModel)\n",
    "# println(\"\\n>>>> CrossValidatorModel.bestModel : \\n\" + cvModel.bestModel)\n",
    "# \n",
    "# //--Cast as PipelineModel.... asInstanceOf[type]....\n",
    "# val pipelineBestModel = cvModel.bestModel.asInstanceOf[PipelineModel]\n",
    "# \n",
    "# println(\"\\n>>>> PipelineModel.stages(0) : \\n\" + pipelineBestModel.stages(0))\n",
    "# \n",
    "# //--Cast as ALSModel.... asInstanceOf[type]....\n",
    "# val alsBestModel = pipelineBestModel.stages(0).asInstanceOf[ALSModel]\n",
    "# \n",
    "# alsBestModel\n",
    "#     .write\n",
    "#     .overwrite\n",
    "#     .save(\"hdfs://spark-master-01:9000/model/audio/profiledata_06-May-2005/als\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "HDFS에 저장된 Best ALSModel을 로딩하여 내용보기"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>>> model.extractParamMap : \n",
      "{Param(parent='als_d46443001501', name='blockSize', doc='block size for stacking input data in matrices. Data is stacked within partitions. If block size is more than remaining data in a partition then it is adjusted to the size of this data.'): 4096, Param(parent='als_d46443001501', name='predictionCol', doc='prediction column name.'): 'prediction', Param(parent='als_d46443001501', name='coldStartStrategy', doc=\"strategy for dealing with unknown or new users/items at prediction time. This may be useful in cross-validation or production scenarios, for handling user/item ids the model has not seen in the training data. Supported values: 'nan', 'drop'.\"): 'drop', Param(parent='als_d46443001501', name='itemCol', doc='column name for item ids. Ids must be within the integer value range.'): 'artistid', Param(parent='als_d46443001501', name='userCol', doc='column name for user ids. Ids must be within the integer value range.'): 'userid'}\n",
      "\n",
      ">>>> model.explainParams : \n",
      "blockSize: block size for stacking input data in matrices. Data is stacked within partitions. If block size is more than remaining data in a partition then it is adjusted to the size of this data. (default: 4096, current: 4096)\n",
      "coldStartStrategy: strategy for dealing with unknown or new users/items at prediction time. This may be useful in cross-validation or production scenarios, for handling user/item ids the model has not seen in the training data. Supported values: 'nan', 'drop'. (current: drop)\n",
      "itemCol: column name for item ids. Ids must be within the integer value range. (current: artistid)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "userCol: column name for user ids. Ids must be within the integer value range. (current: userid)\n"
     ]
    }
   ],
   "source": [
    "#loadedBestALSModel = ALS.load(\"hdfs://spark-master-01:9000/model/audio/profiledata_06-May-2005/als\")\n",
    "loadedBestALSModel = alsModel.load(\"hdfs://localhost:9000/model/audio/profiledata_06-May-2005/als\")\n",
    "\n",
    "print(\"\\n>>>> model.extractParamMap : \\n\" + str(loadedBestALSModel.extractParamMap()))\n",
    "print(\"\\n>>>> model.explainParams : \\n\" + str(loadedBestALSModel.explainParams()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T17:47:00.042267Z",
     "start_time": "2024-03-17T17:46:58.086655Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "로딩된 Best ALSModel로 추천하기 > ALSModel.recommendForUserSubset(DS)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# python\n",
    "userDS2 =spark.createDataFrame([1001440, 2010008, 987654321], IntegerType()).toDF('userID')  # Dataset으로 형변환.... by Encoder....\n",
    "\n",
    "userDS2.printSchema()\n",
    "userDS2.show()\n",
    "\n",
    "# 특정 사용자를 위한 추천 5개\n",
    "recommendedForSomeUsersDF2 = loadedBestALSModel.recommendForUserSubset(userDS2, 5)\n",
    "recommendedForSomeUsersDF2.printSchema()\n",
    "recommendedForSomeUsersDF2.show()\n",
    "\n",
    "# scala\n",
    "# val userDS2 = Seq(1001440, 2010008, 987654321)\n",
    "#     .toDF(\"userID\")\n",
    "#     .as[Int]  //--Dataset으로 형변환.... by Encoder....\n",
    "# \n",
    "# userDS2.printSchema\n",
    "# userDS2.show(false)\n",
    "# \n",
    "# //--특정 사용자를 위한 추천 5개....\n",
    "# val recommendedForSomeUsersDF2 = loadedBestALSModel.recommendForUserSubset(userDS2, 5)\n",
    "# recommendedForSomeUsersDF2.printSchema\n",
    "# recommendedForSomeUsersDF2.show(false)\n",
    "# z.show(recommendedForSomeUsersDF2)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "로딩된 Best ALSModel로 추천하기 > ALSModel.recommendForUserSubset(DS) (w/ 아티스트 이름)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "recommendedForSomeUsersDF2.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# python\n",
    "# explode\n",
    "recommendedForSomeUsersDF3 = recommendedForSomeUsersDF2.withColumn(\"recommend\", F.explode(\"recommendations\")).withColumn(\"artistid\", F.col(\"recommend.artistid\")).withColumn(\"rating\", F.col(\"recommend.rating\"))\n",
    "\n",
    "recommendedForSomeUsersDF3.printSchema()\n",
    "recommendedForSomeUsersDF3.show()\n",
    "print(recommendedForSomeUsersDF3.count())\n",
    "\n",
    "# scala\n",
    "# //--explode....\n",
    "# val recommendedForSomeUsersDF3 = recommendedForSomeUsersDF2\n",
    "#     .withColumn(\"recommend\", explode($\"recommendations\"))\n",
    "#     .withColumn(\"artistid\", $\"recommend.artistid\")\n",
    "#     .withColumn(\"rating\", $\"recommend.rating\")\n",
    "# \n",
    "# recommendedForSomeUsersDF3.printSchema()\n",
    "# recommendedForSomeUsersDF3.show(false)\n",
    "# println(recommendedForSomeUsersDF3.count())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# python\n",
    "# drop\n",
    "recommendedForSomeUsersDF4 = recommendedForSomeUsersDF3.drop(\"recommendations\", \"recommend\")\n",
    "\n",
    "recommendedForSomeUsersDF4.printSchema()\n",
    "recommendedForSomeUsersDF4.show()\n",
    "print(recommendedForSomeUsersDF4.count())\n",
    "\n",
    "# scala\n",
    "# //--drop....\n",
    "# val recommendedForSomeUsersDF4 = recommendedForSomeUsersDF3\n",
    "#     .drop(\"recommendations\", \"recommend\")\n",
    "# \n",
    "# recommendedForSomeUsersDF4.printSchema()\n",
    "# recommendedForSomeUsersDF4.show(false)\n",
    "# println(recommendedForSomeUsersDF4.count())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# python\n",
    "# recommendedForSomeUsersDF33 과 artistFinal df join 하기\n",
    "recommendedForSomeUsersDF5 = recommendedForSomeUsersDF4.join(artistFinal, 'artistid').sort(F.asc('userid'), F.desc('rating')) #spark.sql('select * from recommendedForSomeUsersDF33 as reco join artistFinal as art on reco.artistid = art.artistid').orderBy(F.col(\"userid\").asc(), F.col(\"rating\").desc())\n",
    "\n",
    "recommendedForSomeUsersDF5.show()\n",
    "\n",
    "# scala\n",
    "# //--join w/artistFinal....\n",
    "# val recommendedForSomeUsersDF5 = recommendedForSomeUsersDF4.as(\"reco\")\n",
    "#     .join(artistFinal.as(\"art\"), $\"reco.artistid\" === $\"art.artistid\")\n",
    "#     .orderBy($\"userid\".asc, $\"rating\".desc)\n",
    "# \n",
    "# recommendedForSomeUsersDF5.show(false)\n",
    "# z.show(recommendedForSomeUsersDF5)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Redis] 모든 사용자를 위한 아티스트 5건 추천 #1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userid: integer (nullable = false)\n",
      " |-- recommendations: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- artistid: integer (nullable = true)\n",
      " |    |    |-- rating: float (nullable = true)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7459:>                                                     (0 + 10) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|userid|     recommendations|\n",
      "+------+--------------------+\n",
      "|   321|[{1000114, 0.0071...|\n",
      "|   350|[{1000114, 0.0634...|\n",
      "|   581|[{1000114, 0.1034...|\n",
      "|  1197|[{1205, 0.2998680...|\n",
      "|  3208|[{82, 0.5576205},...|\n",
      "|  3793|[{1000114, 0.0037...|\n",
      "|  5595|[{82, 7.5130665E-...|\n",
      "|  5868|[{1016335, 0.0012...|\n",
      "|  6043|[{1205, 0.5031999...|\n",
      "|  6116|[{1000114, 0.0816...|\n",
      "|  6676|[{1008419, 0.7271...|\n",
      "|  6865|[{1000114, 1.5179...|\n",
      "|  6937|[{1205, 0.6262252...|\n",
      "|  7333|[{1129433, 5.0424...|\n",
      "|  7340|[{1000114, 5.0044...|\n",
      "|  7584|[{82, 0.2385274},...|\n",
      "|  7806|[{82, 0.84356815}...|\n",
      "|  8492|[{1001007, 1.3225...|\n",
      "|  8815|[{82, 4.065707E-1...|\n",
      "|  9058|[{1205, 1.9128599...|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "<bound method DataFrame.count of DataFrame[userid: int, recommendations: array<struct<artistid:int,rating:float>>]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# python\n",
    "recommendedForAllUsersDF = loadedBestALSModel.recommendForAllUsers(5)\n",
    "\n",
    "recommendedForAllUsersDF.printSchema()\n",
    "recommendedForAllUsersDF.show()\n",
    "print(recommendedForAllUsersDF.count)\n",
    "\n",
    "# scala\n",
    "# val recommendedForAllUsersDF = loadedBestALSModel.recommendForAllUsers(5)\n",
    "# \n",
    "# recommendedForAllUsersDF.printSchema\n",
    "# recommendedForAllUsersDF.show(false)\n",
    "# println(recommendedForAllUsersDF.count)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T17:47:36.145464Z",
     "start_time": "2024-03-17T17:47:22.178249Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Redis] 모든 사용자를 위한 아티스트 5건 추천 #2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userid: integer (nullable = false)\n",
      " |-- recommendations: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- artistid: integer (nullable = true)\n",
      " |    |    |-- rating: float (nullable = true)\n",
      " |-- recommendation: struct (nullable = true)\n",
      " |    |-- artistid: integer (nullable = true)\n",
      " |    |-- rating: float (nullable = true)\n",
      " |-- artistid: integer (nullable = true)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7463:============================================>          (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+--------+\n",
      "|userid|     recommendations|      recommendation|artistid|\n",
      "+------+--------------------+--------------------+--------+\n",
      "|   321|[{1000114, 0.0071...|{1000114, 0.00713...| 1000114|\n",
      "|   321|[{1000114, 0.0071...|{1008419, 0.00528...| 1008419|\n",
      "|   321|[{1000114, 0.0071...|  {599, 0.005223873}|     599|\n",
      "|   321|[{1000114, 0.0071...|{1001779, 0.00448...| 1001779|\n",
      "|   321|[{1000114, 0.0071...|{1077309, 0.00432...| 1077309|\n",
      "|   350|[{1000114, 0.0634...|{1000114, 0.06348...| 1000114|\n",
      "|   350|[{1000114, 0.0634...|{1001007, 0.05670...| 1001007|\n",
      "|   350|[{1000114, 0.0634...|  {1854, 0.05076628}|    1854|\n",
      "|   350|[{1000114, 0.0634...|{1001779, 0.04903...| 1001779|\n",
      "|   350|[{1000114, 0.0634...|{1002704, 0.04882...| 1002704|\n",
      "|   581|[{1000114, 0.1034...|{1000114, 0.1034589}| 1000114|\n",
      "|   581|[{1000114, 0.1034...|{1008419, 0.08009...| 1008419|\n",
      "|   581|[{1000114, 0.1034...|   {599, 0.07628642}|     599|\n",
      "|   581|[{1000114, 0.1034...|  {1259, 0.06451957}|    1259|\n",
      "|   581|[{1000114, 0.1034...|{1001779, 0.06284...| 1001779|\n",
      "|  1197|[{1205, 0.2998680...|  {1205, 0.29986805}|    1205|\n",
      "|  1197|[{1205, 0.2998680...|   {930, 0.26412898}|     930|\n",
      "|  1197|[{1205, 0.2998680...|  {1991, 0.24941848}|    1991|\n",
      "|  1197|[{1205, 0.2998680...|{1305754, 0.2292318}| 1305754|\n",
      "|  1197|[{1205, 0.2998680...|{1001007, 0.22051...| 1001007|\n",
      "+------+--------------------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "<bound method DataFrame.count of DataFrame[userid: int, recommendations: array<struct<artistid:int,rating:float>>, recommendation: struct<artistid:int,rating:float>, artistid: int]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# python\n",
    "recommendedForAllUsersDF2 = recommendedForAllUsersDF.withColumn(\"recommendation\", F.explode(\"recommendations\")).withColumn(\"artistid\", F.col(\"recommendation.artistid\"))\n",
    "    \n",
    "recommendedForAllUsersDF2.printSchema()\n",
    "recommendedForAllUsersDF2.show()\n",
    "print(recommendedForAllUsersDF2.count)\n",
    "\n",
    "# scala\n",
    "# val recommendedForAllUsersDF2 = recommendedForAllUsersDF\n",
    "#     .withColumn(\"recommendation\", explode($\"recommendations\"))\n",
    "#     .withColumn(\"artistid\", $\"recommendation.artistid\")\n",
    "#     \n",
    "# recommendedForAllUsersDF2.printSchema\n",
    "# recommendedForAllUsersDF2.show(false)\n",
    "# println(recommendedForAllUsersDF2.count)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T17:47:43.395251Z",
     "start_time": "2024-03-17T17:47:36.146589Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Redis] 모든 사용자를 위한 아티스트 5건 추천 #3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userid: integer (nullable = false)\n",
      " |-- artistid_list: array (nullable = false)\n",
      " |    |-- element: integer (containsNull = false)\n",
      " |-- recommended_artistids: string (nullable = false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7469:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+---------------------+\n",
      "|userid|       artistid_list|recommended_artistids|\n",
      "+------+--------------------+---------------------+\n",
      "|   321|[1000114, 1008419...| 1000114 1008419 5...|\n",
      "|   350|[1000114, 1001007...| 1000114 1001007 1...|\n",
      "|   581|[1000114, 1008419...| 1000114 1008419 5...|\n",
      "|  1197|[1205, 930, 1991,...| 1205 930 1991 130...|\n",
      "|  3208|[82, 1001412, 100...| 82 1001412 100032...|\n",
      "|  3793|[1000114, 1008419...| 1000114 1008419 5...|\n",
      "|  5595|[82, 1001412, 112...| 82 1001412 112943...|\n",
      "|  5868|[1016335, 1233196...| 1016335 1233196 1...|\n",
      "|  6043|[1205, 1991, 1129...| 1205 1991 1129433...|\n",
      "|  6116|[1000114, 1001007...| 1000114 1001007 1...|\n",
      "|  6676|[1008419, 1000263...| 1008419 1000263 2...|\n",
      "|  6865|[1000114, 1008419...| 1000114 1008419 1...|\n",
      "|  6937|[1205, 930, 10010...| 1205 930 1001007 ...|\n",
      "|  7333|[1129433, 1233770...| 1129433 1233770 1...|\n",
      "|  7340|[1000114, 1008419...| 1000114 1008419 5...|\n",
      "|  7584|[82, 1001412, 110...| 82 1001412 110810...|\n",
      "|  7806|[82, 1001412, 110...| 82 1001412 110810...|\n",
      "|  8492|[1001007, 930, 12...| 1001007 930 1205 ...|\n",
      "|  8815|[82, 1007903, 100...| 82 1007903 100032...|\n",
      "|  9058|[1205, 1991, 930,...| 1205 1991 930 130...|\n",
      "+------+--------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "<bound method DataFrame.count of DataFrame[userid: int, artistid_list: array<int>, recommended_artistids: string]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# python\n",
    "recommendedForAllUsersDF3 = recommendedForAllUsersDF2.groupBy(\"userid\").agg(F.collect_list(\"artistid\").alias(\"artistid_list\")).withColumn(\"recommended_artistids\", F.array_join(F.col(\"artistid_list\"), \" \"))\n",
    "    \n",
    "recommendedForAllUsersDF3.printSchema()\n",
    "recommendedForAllUsersDF3.show()\n",
    "print(recommendedForAllUsersDF3.count)\n",
    "\n",
    "# scala\n",
    "# val recommendedForAllUsersDF3 = recommendedForAllUsersDF2\n",
    "#     .groupBy(\"userid\")\n",
    "#     .agg(collect_list(\"artistid\").as(\"artistid_list\"))\n",
    "#     .withColumn(\"recommended_artistids\", array_join(col(\"artistid_list\"), \" \"))\n",
    "#     \n",
    "# recommendedForAllUsersDF3.printSchema\n",
    "# recommendedForAllUsersDF3.show(false)\n",
    "# println(recommendedForAllUsersDF3.count)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T17:47:52.769082Z",
     "start_time": "2024-03-17T17:47:43.786440Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Redis] 모든 사용자를 위한 아티스트 5건 추천 #4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userid: integer (nullable = false)\n",
      " |-- recommended_artistids: string (nullable = false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7471:>                                                     (0 + 10) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------------+\n",
      "|userid|recommended_artistids|\n",
      "+------+---------------------+\n",
      "|   321| 1000114 1008419 5...|\n",
      "|   350| 1000114 1001007 1...|\n",
      "|   581| 1000114 1008419 5...|\n",
      "|  1197| 1205 930 1991 130...|\n",
      "|  3208| 82 1001412 100032...|\n",
      "|  3793| 1000114 1008419 5...|\n",
      "|  5595| 82 1001412 112943...|\n",
      "|  5868| 1016335 1233196 1...|\n",
      "|  6043| 1205 1991 1129433...|\n",
      "|  6116| 1000114 1001007 1...|\n",
      "|  6676| 1008419 1000263 2...|\n",
      "|  6865| 1000114 1008419 1...|\n",
      "|  6937| 1205 930 1001007 ...|\n",
      "|  7333| 1129433 1233770 1...|\n",
      "|  7340| 1000114 1008419 5...|\n",
      "|  7584| 82 1001412 110810...|\n",
      "|  7806| 82 1001412 110810...|\n",
      "|  8492| 1001007 930 1205 ...|\n",
      "|  8815| 82 1007903 100032...|\n",
      "|  9058| 1205 1991 930 130...|\n",
      "+------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "<bound method DataFrame.count of DataFrame[userid: int, recommended_artistids: string]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# python\n",
    "recommendedForAllUsersDF4 = recommendedForAllUsersDF3.select( 'userid', 'recommended_artistids' )\n",
    "\n",
    "recommendedForAllUsersDF4.printSchema()\n",
    "recommendedForAllUsersDF4.show()\n",
    "print(recommendedForAllUsersDF4.count)\n",
    "\n",
    "# scala\n",
    "# val recommendedForAllUsersDF4 = recommendedForAllUsersDF3.select($\"userid\", $\"recommended_artistids\")\n",
    "# recommendedForAllUsersDF4.printSchema\n",
    "# recommendedForAllUsersDF4.show(false)\n",
    "# println(recommendedForAllUsersDF4.count)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T17:48:03.416107Z",
     "start_time": "2024-03-17T17:47:55.422466Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Redis] 모든 사용자를 위한 아티스트 5건 추천 저장"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o11485.save.\n: redis.clients.jedis.exceptions.JedisConnectionException: Could not get a resource from the pool\n\tat redis.clients.jedis.util.Pool.getResource(Pool.java:84)\n\tat redis.clients.jedis.JedisPool.getResource(JedisPool.java:377)\n\tat com.redislabs.provider.redis.ConnectionPool$.connect(ConnectionPool.scala:35)\n\tat com.redislabs.provider.redis.RedisEndpoint.connect(RedisConfig.scala:94)\n\tat com.redislabs.provider.redis.RedisConfig.clusterEnabled(RedisConfig.scala:227)\n\tat com.redislabs.provider.redis.RedisConfig.getNodes(RedisConfig.scala:367)\n\tat com.redislabs.provider.redis.RedisConfig.getHosts(RedisConfig.scala:267)\n\tat com.redislabs.provider.redis.RedisConfig.<init>(RedisConfig.scala:166)\n\tat com.redislabs.provider.redis.RedisConfig$.fromSparkConfAndParameters(RedisConfig.scala:154)\n\tat org.apache.spark.sql.redis.RedisSourceRelation.<init>(RedisSourceRelation.scala:34)\n\tat org.apache.spark.sql.redis.DefaultSource.createRelation(DefaultSource.scala:21)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: redis.clients.jedis.exceptions.JedisConnectionException: Failed to create socket.\n\tat redis.clients.jedis.DefaultJedisSocketFactory.createSocket(DefaultJedisSocketFactory.java:110)\n\tat redis.clients.jedis.Connection.connect(Connection.java:226)\n\tat redis.clients.jedis.BinaryClient.connect(BinaryClient.java:144)\n\tat redis.clients.jedis.BinaryJedis.connect(BinaryJedis.java:314)\n\tat redis.clients.jedis.BinaryJedis.initializeFromClientConfig(BinaryJedis.java:92)\n\tat redis.clients.jedis.BinaryJedis.<init>(BinaryJedis.java:297)\n\tat redis.clients.jedis.Jedis.<init>(Jedis.java:169)\n\tat redis.clients.jedis.JedisFactory.makeObject(JedisFactory.java:177)\n\tat org.apache.commons.pool2.impl.GenericObjectPool.create(GenericObjectPool.java:571)\n\tat org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:298)\n\tat org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:223)\n\tat redis.clients.jedis.util.Pool.getResource(Pool.java:75)\n\t... 51 more\nCaused by: java.net.ConnectException: Connection refused (Connection refused)\n\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n\tat java.net.Socket.connect(Socket.java:607)\n\tat redis.clients.jedis.DefaultJedisSocketFactory.createSocket(DefaultJedisSocketFactory.java:80)\n\t... 62 more\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mrecommendedForAllUsersDF4\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43morg.apache.spark.sql.redis\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtable\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43muser_artists2\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mkey.column\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43muserid\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mappend\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/project_Recom_Music_Spark/venv/lib/python3.9/site-packages/pyspark/sql/readwriter.py:738\u001B[0m, in \u001B[0;36mDataFrameWriter.save\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n\u001B[1;32m    736\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mformat\u001B[39m)\n\u001B[1;32m    737\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 738\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    739\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    740\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave(path)\n",
      "File \u001B[0;32m~/project_Recom_Music_Spark/venv/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
      "File \u001B[0;32m~/project_Recom_Music_Spark/venv/lib/python3.9/site-packages/pyspark/sql/utils.py:111\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    109\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw):\n\u001B[1;32m    110\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 111\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    112\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m py4j\u001B[38;5;241m.\u001B[39mprotocol\u001B[38;5;241m.\u001B[39mPy4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    113\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
      "File \u001B[0;32m~/project_Recom_Music_Spark/venv/lib/python3.9/site-packages/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
      "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o11485.save.\n: redis.clients.jedis.exceptions.JedisConnectionException: Could not get a resource from the pool\n\tat redis.clients.jedis.util.Pool.getResource(Pool.java:84)\n\tat redis.clients.jedis.JedisPool.getResource(JedisPool.java:377)\n\tat com.redislabs.provider.redis.ConnectionPool$.connect(ConnectionPool.scala:35)\n\tat com.redislabs.provider.redis.RedisEndpoint.connect(RedisConfig.scala:94)\n\tat com.redislabs.provider.redis.RedisConfig.clusterEnabled(RedisConfig.scala:227)\n\tat com.redislabs.provider.redis.RedisConfig.getNodes(RedisConfig.scala:367)\n\tat com.redislabs.provider.redis.RedisConfig.getHosts(RedisConfig.scala:267)\n\tat com.redislabs.provider.redis.RedisConfig.<init>(RedisConfig.scala:166)\n\tat com.redislabs.provider.redis.RedisConfig$.fromSparkConfAndParameters(RedisConfig.scala:154)\n\tat org.apache.spark.sql.redis.RedisSourceRelation.<init>(RedisSourceRelation.scala:34)\n\tat org.apache.spark.sql.redis.DefaultSource.createRelation(DefaultSource.scala:21)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: redis.clients.jedis.exceptions.JedisConnectionException: Failed to create socket.\n\tat redis.clients.jedis.DefaultJedisSocketFactory.createSocket(DefaultJedisSocketFactory.java:110)\n\tat redis.clients.jedis.Connection.connect(Connection.java:226)\n\tat redis.clients.jedis.BinaryClient.connect(BinaryClient.java:144)\n\tat redis.clients.jedis.BinaryJedis.connect(BinaryJedis.java:314)\n\tat redis.clients.jedis.BinaryJedis.initializeFromClientConfig(BinaryJedis.java:92)\n\tat redis.clients.jedis.BinaryJedis.<init>(BinaryJedis.java:297)\n\tat redis.clients.jedis.Jedis.<init>(Jedis.java:169)\n\tat redis.clients.jedis.JedisFactory.makeObject(JedisFactory.java:177)\n\tat org.apache.commons.pool2.impl.GenericObjectPool.create(GenericObjectPool.java:571)\n\tat org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:298)\n\tat org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:223)\n\tat redis.clients.jedis.util.Pool.getResource(Pool.java:75)\n\t... 51 more\nCaused by: java.net.ConnectException: Connection refused (Connection refused)\n\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n\tat java.net.Socket.connect(Socket.java:607)\n\tat redis.clients.jedis.DefaultJedisSocketFactory.createSocket(DefaultJedisSocketFactory.java:80)\n\t... 62 more\n"
     ]
    }
   ],
   "source": [
    "recommendedForAllUsersDF4.write.format(\"org.apache.spark.sql.redis\").option(\"table\", \"user_artists2\").option(\"key.column\", \"userid\").mode(\"append\").save()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T08:52:10.908218Z",
     "start_time": "2024-03-18T08:52:08.769660Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Redis] 모든 사용자를 위한 아티스트 5건 추천 조회"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o11943.load.\n: redis.clients.jedis.exceptions.JedisConnectionException: Could not get a resource from the pool\n\tat redis.clients.jedis.util.Pool.getResource(Pool.java:84)\n\tat redis.clients.jedis.JedisPool.getResource(JedisPool.java:377)\n\tat com.redislabs.provider.redis.ConnectionPool$.connect(ConnectionPool.scala:35)\n\tat com.redislabs.provider.redis.RedisEndpoint.connect(RedisConfig.scala:94)\n\tat com.redislabs.provider.redis.RedisConfig.clusterEnabled(RedisConfig.scala:227)\n\tat com.redislabs.provider.redis.RedisConfig.getNodes(RedisConfig.scala:367)\n\tat com.redislabs.provider.redis.RedisConfig.getHosts(RedisConfig.scala:267)\n\tat com.redislabs.provider.redis.RedisConfig.<init>(RedisConfig.scala:166)\n\tat com.redislabs.provider.redis.RedisConfig$.fromSparkConfAndParameters(RedisConfig.scala:154)\n\tat org.apache.spark.sql.redis.RedisSourceRelation.<init>(RedisSourceRelation.scala:34)\n\tat org.apache.spark.sql.redis.DefaultSource.createRelation(DefaultSource.scala:13)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: redis.clients.jedis.exceptions.JedisConnectionException: Failed to create socket.\n\tat redis.clients.jedis.DefaultJedisSocketFactory.createSocket(DefaultJedisSocketFactory.java:110)\n\tat redis.clients.jedis.Connection.connect(Connection.java:226)\n\tat redis.clients.jedis.BinaryClient.connect(BinaryClient.java:144)\n\tat redis.clients.jedis.BinaryJedis.connect(BinaryJedis.java:314)\n\tat redis.clients.jedis.BinaryJedis.initializeFromClientConfig(BinaryJedis.java:92)\n\tat redis.clients.jedis.BinaryJedis.<init>(BinaryJedis.java:297)\n\tat redis.clients.jedis.Jedis.<init>(Jedis.java:169)\n\tat redis.clients.jedis.JedisFactory.makeObject(JedisFactory.java:177)\n\tat org.apache.commons.pool2.impl.GenericObjectPool.create(GenericObjectPool.java:571)\n\tat org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:298)\n\tat org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:223)\n\tat redis.clients.jedis.util.Pool.getResource(Pool.java:75)\n\t... 28 more\nCaused by: java.net.ConnectException: Connection refused (Connection refused)\n\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n\tat java.net.Socket.connect(Socket.java:607)\n\tat redis.clients.jedis.DefaultJedisSocketFactory.createSocket(DefaultJedisSocketFactory.java:80)\n\t... 39 more\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# python\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m recommendedForAllUsersDF5 \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43morg.apache.spark.sql.redis\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtable\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43muser_artists\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moption\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mkey.column\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43muserid\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m recommendedForAllUsersDF5\u001B[38;5;241m.\u001B[39mprintSchema()\n\u001B[1;32m      5\u001B[0m recommendedForAllUsersDF5\u001B[38;5;241m.\u001B[39mshow()\n",
      "File \u001B[0;32m~/project_Recom_Music_Spark/venv/lib/python3.9/site-packages/pyspark/sql/readwriter.py:164\u001B[0m, in \u001B[0;36mDataFrameReader.load\u001B[0;34m(self, path, format, schema, **options)\u001B[0m\n\u001B[1;32m    162\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jreader\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39m_sc\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mPythonUtils\u001B[38;5;241m.\u001B[39mtoSeq(path)))\n\u001B[1;32m    163\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 164\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jreader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/project_Recom_Music_Spark/venv/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
      "File \u001B[0;32m~/project_Recom_Music_Spark/venv/lib/python3.9/site-packages/pyspark/sql/utils.py:111\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    109\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw):\n\u001B[1;32m    110\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 111\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    112\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m py4j\u001B[38;5;241m.\u001B[39mprotocol\u001B[38;5;241m.\u001B[39mPy4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    113\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
      "File \u001B[0;32m~/project_Recom_Music_Spark/venv/lib/python3.9/site-packages/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
      "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o11943.load.\n: redis.clients.jedis.exceptions.JedisConnectionException: Could not get a resource from the pool\n\tat redis.clients.jedis.util.Pool.getResource(Pool.java:84)\n\tat redis.clients.jedis.JedisPool.getResource(JedisPool.java:377)\n\tat com.redislabs.provider.redis.ConnectionPool$.connect(ConnectionPool.scala:35)\n\tat com.redislabs.provider.redis.RedisEndpoint.connect(RedisConfig.scala:94)\n\tat com.redislabs.provider.redis.RedisConfig.clusterEnabled(RedisConfig.scala:227)\n\tat com.redislabs.provider.redis.RedisConfig.getNodes(RedisConfig.scala:367)\n\tat com.redislabs.provider.redis.RedisConfig.getHosts(RedisConfig.scala:267)\n\tat com.redislabs.provider.redis.RedisConfig.<init>(RedisConfig.scala:166)\n\tat com.redislabs.provider.redis.RedisConfig$.fromSparkConfAndParameters(RedisConfig.scala:154)\n\tat org.apache.spark.sql.redis.RedisSourceRelation.<init>(RedisSourceRelation.scala:34)\n\tat org.apache.spark.sql.redis.DefaultSource.createRelation(DefaultSource.scala:13)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: redis.clients.jedis.exceptions.JedisConnectionException: Failed to create socket.\n\tat redis.clients.jedis.DefaultJedisSocketFactory.createSocket(DefaultJedisSocketFactory.java:110)\n\tat redis.clients.jedis.Connection.connect(Connection.java:226)\n\tat redis.clients.jedis.BinaryClient.connect(BinaryClient.java:144)\n\tat redis.clients.jedis.BinaryJedis.connect(BinaryJedis.java:314)\n\tat redis.clients.jedis.BinaryJedis.initializeFromClientConfig(BinaryJedis.java:92)\n\tat redis.clients.jedis.BinaryJedis.<init>(BinaryJedis.java:297)\n\tat redis.clients.jedis.Jedis.<init>(Jedis.java:169)\n\tat redis.clients.jedis.JedisFactory.makeObject(JedisFactory.java:177)\n\tat org.apache.commons.pool2.impl.GenericObjectPool.create(GenericObjectPool.java:571)\n\tat org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:298)\n\tat org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:223)\n\tat redis.clients.jedis.util.Pool.getResource(Pool.java:75)\n\t... 28 more\nCaused by: java.net.ConnectException: Connection refused (Connection refused)\n\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n\tat java.net.Socket.connect(Socket.java:607)\n\tat redis.clients.jedis.DefaultJedisSocketFactory.createSocket(DefaultJedisSocketFactory.java:80)\n\t... 39 more\n"
     ]
    }
   ],
   "source": [
    "# python\n",
    "recommendedForAllUsersDF5 = spark.read.format(\"org.apache.spark.sql.redis\").option(\"table\", \"user_artists\").option(\"key.column\", \"userid\").load()\n",
    "\n",
    "recommendedForAllUsersDF5.printSchema()\n",
    "recommendedForAllUsersDF5.show()\n",
    "print(recommendedForAllUsersDF5.count)\n",
    "\n",
    "# scala\n",
    "# val recommendedForAllUsersDF5 = spark\n",
    "#     .read\n",
    "#     .format(\"org.apache.spark.sql.redis\")\n",
    "#     .option(\"table\", \"user_artists\")\n",
    "#     .option(\"key.column\", \"userid\")\n",
    "#     .load()\n",
    "# \n",
    "# recommendedForAllUsersDF5.printSchema\n",
    "# recommendedForAllUsersDF5.show(false)\n",
    "# println(recommendedForAllUsersDF5.count)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T17:57:29.558278Z",
     "start_time": "2024-03-17T17:57:29.453020Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Redis] 특정 사용자를 위한 아티스트 5건 추천 조회"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# python\n",
    "recommendedForAllUsersDF4 = spark.read.format(\"org.apache.spark.sql.redis\").option(\"keys.pattern\", \"user_artists:2127894\").option(\"key.column\", \"userid\").option(\"infer.schema\", True).load()\n",
    "\n",
    "recommendedForAllUsersDF4.printSchema()\n",
    "recommendedForAllUsersDF4.show()\n",
    "\n",
    "# scala\n",
    "# val recommendedForAllUsersDF4 = spark\n",
    "#     .read\n",
    "#     .format(\"org.apache.spark.sql.redis\")\n",
    "#     .option(\"keys.pattern\", \"user_artists:2127894\")\n",
    "#     .option(\"key.column\", \"userid\")\n",
    "#     .option(\"infer.schema\", true)\n",
    "#     .load()\n",
    "# \n",
    "# recommendedForAllUsersDF4.printSchema\n",
    "# recommendedForAllUsersDF4.show(false)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
